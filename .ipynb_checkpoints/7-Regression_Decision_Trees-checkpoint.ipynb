{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c2b86ee-dbfc-466a-9bc4-9ab94c1c22c2",
   "metadata": {},
   "source": [
    "# Tree-Based Methods\n",
    "\n",
    "After a first taste of machine learning modeling with simple linear models that are common also to econometrics, we now dive into the **Tree-Based Methods**. These ML models are powerful and largely employed in the financial space. Our strategy to introduce those models is:\n",
    "\n",
    "- *Explore* the underlying reasoning of a tree-based model and observe a simple application outside the financial domains.\n",
    "\n",
    "- *Extend* the use ofthese tree-based models through some powerful algorithms and see a couple of financial applications (in the next lecture).\n",
    "\n",
    "It is always important to follow these steps: *first* understand the model on basic problems, *then* complicate it and adapt to a specific domain. We always remember that finance comes with its own challenges. If you start directly from a finance use-cases, you may lost some characteristics of the model.\n",
    "\n",
    "## Background theory\n",
    "\n",
    "\n",
    "Tree-based methods can take your dataset, partition it into different sets and fit a simple model to each partition. A very popular method for tree-based model is called **CART**, which stands for *classification and regression tree*. As the name suggest trees can do either *regression* or *classification* with slight changes.\n",
    "\n",
    "Consider a simple regression problem where we have a $Y$ and two inputs $X_{1}$ and $X_{2}$ that takes value in the unit interval. *Tree-based methods* construct binary partition of the feature space.\n",
    "\n",
    "<img src=\"images/binary_split.png\" width=\"400\">\n",
    "\n",
    "    \n",
    "<img src=\"images/tree_split.png\" width=\"400\">\n",
    "\n",
    "\n",
    "In each binary partition element we can model $Y$ with the mean of $Y$ in each region. *We choose the variable and split-point to achieve the best split*. The result of this process is a partition into the five regions $R_{1}, R_{2}, \\ldots, R_{5}$. The corresponding regression model predicts $Y$ with a constant $c_{m}$ in region $R_{m}$, that is,\n",
    "$$\n",
    "\\hat{f}(X)=\\sum_{m=1}^{5} c_{m} I\\left\\{\\left(X_{1}, X_{2}\\right) \\in R_{m}\\right\\}\n",
    "$$\n",
    "\n",
    "**Every tree works like this**, either you are solving a classification or a regression problem. *The full dataset sits at the top of the tree*. Observations satisfying the condition at each junction are assigned to the left branch, and the others to the right branch. The terminal nodes or leaves of the tree correspond to the regions $R_{1}, R_{2}, \\ldots, R_{5}$. \n",
    "\n",
    "\n",
    "**Why do we consider these apparently simple partition so powerful?**\n",
    "\n",
    "It is all about **interpretability**. Binary trees are easier to interpret, even though would be hard to draw the scheme of the tree with more than two inputs, which is often the case in many applications.\n",
    "\n",
    "## Regression tree\n",
    "\n",
    "The first important question is *how to grow a regression tree*. The data consists of $p$ inputs and a response, for each of $N$ observations: that is, $\\left(x_{i}, y_{i}\\right)$ for $i=1,2, \\ldots, N$, with $x_{i}=\\left(x_{i 1}, x_{i 2}, \\ldots, x_{i p}\\right)$. The algorithms needs to automatically decide on *splitting variables* and *split points*, other than the shape of the tree itself (how many nodes and leaves).\n",
    "\n",
    "Starting with all of the data, consider a splitting variable $j$ and split point $s$, and define the pair of half-planes\n",
    "$$\n",
    "R_{1}(j, s)=\\left\\{X \\mid X_{j} \\leq s\\right\\} \\text { and } R_{2}(j, s)=\\left\\{X \\mid X_{j}>s\\right\\} .\n",
    "$$\n",
    "Then we seek the splitting variable $j$ and split point $s$ that solve\n",
    "$$\n",
    "\\min _{j, s}\\left[\\min _{c_{1}} \\sum_{x_{i} \\in R_{1}(j, s)}\\left(y_{i}-c_{1}\\right)^{2}+\\min _{c_{2}} \\sum_{x_{i} \\in R_{2}(j, s)}\\left(y_{i}-c_{2}\\right)^{2}\\right] .\n",
    "$$\n",
    "For any choice $j$ and $s$, the inner minimization is solved by\n",
    "$$\n",
    "\\hat{c}_{1}=\\operatorname{ave}\\left(y_{i} \\mid x_{i} \\in R_{1}(j, s)\\right) \\text { and } \\hat{c}_{2}=\\operatorname{ave}\\left(y_{i} \\mid x_{i} \\in R_{2}(j, s)\\right) .\n",
    "$$\n",
    "For each splitting variable, the determination of the split point $s$ can be done very quickly and hence by scanning through all of the inputs, determination of the best pair $(j, s)$ is feasible.\n",
    "\n",
    "**Having found the best split, we partition the data into the two resulting regions and repeat the splitting process on each of the two regions.** This iterative process is at the core of the tree-based mechanisms and is repeated on all of the resulting regions.\n",
    "\n",
    "\n",
    "**How large should we grow the tree?** A very large tree might **overfit** (*remember?*) the data, while a small tree might not capture the important structure.\n",
    "\n",
    "*The size of the tree* is a parameter to tune and regulates the model complexity.\n",
    "\n",
    "\n",
    "*One approach would be to split tree nodes only if the decrease in sum-of-squares due to the split exceeds some threshold. This strategy is too **short-sighted**, however, since a seemingly worthless split might lead to a very good split below it.* \n",
    "\n",
    "**What's the strategy to adopt then?**\n",
    "\n",
    "- Grow a large tree $T_{0}$ by stopping the split only when a minimum number of node is reached.\n",
    "- Prune (cut the non-terminal branches) using a cost-complexity criteria to get the best compromise between size and representational power.\n",
    "\n",
    "**Cost-complexity criteria**\n",
    "\n",
    "We define a subtree $T \\subset T_{0}$ to be any tree that can be obtained by pruning $T_{0}$, that is, collapsing any number of its internal (non-terminal) nodes. We index terminal nodes by $m$, with node $m$ representing region $R_{m}$. Let $|T|$ denote the number of terminal nodes in $T$. Letting\n",
    "$$\n",
    "\\begin{aligned}\n",
    "N_{m} &=\\#\\left\\{x_{i} \\in R_{m}\\right\\}, \\\\\n",
    "\\hat{c}_{m} &=\\frac{1}{N_{m}} \\sum_{x_{i} \\in R_{m}} y_{i} \\\\\n",
    "Q_{m}(T) &=\\frac{1}{N_{m}} \\sum_{x_{i} \\in R_{m}}\\left(y_{i}-\\hat{c}_{m}\\right)^{2}\n",
    "\\end{aligned}\n",
    "$$\n",
    "we define the cost complexity criterion\n",
    "$$\n",
    "C_{\\alpha}(T)=\\sum_{m=1}^{|T|} N_{m} Q_{m}(T)+\\alpha|T|\n",
    "$$\n",
    "\n",
    "*The idea is to find, for each $\\alpha$, the subtree $T_{\\alpha} \\subseteq T_{0}$ to minimize $C_{\\alpha}(T)$*. \n",
    "\n",
    "**The parameter $\\alpha \\geq 0$ need to be tuned and governs the tradeoff between tree size and its goodness of fit to the data.**\n",
    "\n",
    "- Large values of $\\alpha$ result in smaller trees. \n",
    "- Small values of $\\alpha$ result in bigger trees.\n",
    "- $\\alpha=0$ results in the full tree $T_{0}$. \n",
    "\n",
    "\n",
    "For each $\\alpha$ one can show that there is a unique smallest subtree $T_{\\alpha}$ that minimizes $C_{\\alpha}(T)$. To find $T_{\\alpha}$ we use weakest link pruning: we successively collapse the internal node that produces the smallest per-node increase in $\\sum_{m} N_{m} Q_{m}(T)$, and continue until we produce the single-node (root) tree. \n",
    "\n",
    "Estimation of $\\alpha$ is achieved by cross-validation: we choose the value $\\hat{\\alpha}$ to minimize the cross-validated sum of squares. \n",
    "\n",
    "\n",
    "## Classification tree\n",
    "\n",
    "The substantial difference between a regression tree and a classification tree (as many other algorithms that can do both task) is in the type of outcome values. While for the former the outcome is a real value, for the latter is a discrete set of values $1,2, \\ldots, K$. This means that the only change needed is the criteria for splitting and pruning. The impurity measure $Q_{m}(T)$  used above for regression tree is not useful anymore.\n",
    "\n",
    "Define the proportion of class $k$ observations in node $m$, representing a region $R_{m}$ with $N_{m}$ observations, as\n",
    "$$\n",
    "\\hat{p}_{m k}=\\frac{1}{N_{m}} \\sum_{x_{i} \\in R_{m}} I\\left(y_{i}=k\\right),\n",
    "$$\n",
    "\n",
    "We can classify the observations in node $m$ to class $k(m)=\\arg \\max _{k} \\hat{p}_{m k}$, the majority class in node $m$. Useful measures $Q_{m}(T)$ of node impurity include the following:\n",
    "\n",
    "\n",
    "$$\\begin{array}{ll}\\text { Misclassification error: } & \\frac{1}{N_{m}} \\sum_{i \\in R_{m}} I\\left(y_{i} \\neq k(m)\\right)=1-\\hat{p}_{m k(m)} . \\\\ \\text { Gini index: } & \\sum_{k \\neq k^{\\prime}} \\hat{p}_{m k} \\hat{p}_{m k^{\\prime}}=\\sum_{k=1}^{K} \\hat{p}_{m k}\\left(1-\\hat{p}_{m k}\\right) . \\\\ \\text { Cross-entropy or deviance: } & -\\sum_{k=1}^{K} \\hat{p}_{m k} \\log \\hat{p}_{m k} .\\end{array}$$\n",
    "\n",
    "<img src=\"images/class_tree_metrics.png\" width=\"400\">\n",
    "\n",
    "For two classes, if $p$ is the proportion in the second class, these three measures are $1-\\max (p, 1-p), 2 p(1-p)$ and $-p \\log p-(1-p) \\log (1-p)$, respectively. All three are similar, but crossentropy and the Gini index are differentiable (look at the figure), and hence more tractable with numerical optimization. \n",
    "\n",
    "*For this reason, either the Gini index or cross-entropy should be used when growing the tree. To guide cost-complexity pruning, any of the three measures can be used, but typically it is the misclassification rate.* Refer to the book for the reason of this choice.\n",
    "\n",
    "The Gini index can be interpreted in two interesting ways. Rather than classify observations to the majority class in the node, we could classify them to class $k$ with probability $\\hat{p}_{m k}$. Then the expected training error rate of this rule in the node is $\\sum_{k \\neq k^{\\prime}} \\hat{p}_{m k} \\hat{p}_{m k^{\\prime}}$ - the Gini index. Similarly, if we code each observation as 1 for class $k$ and zero otherwise, the variance over the node of this 0 - 1 response is $\\hat{p}_{m k}\\left(1-\\hat{p}_{m k}\\right)$. Summing over classes $k$ again gives the Gini index.\n",
    "\n",
    "**Let's look at some code now**. We will see how to use these models on some toy problems.\n",
    "\n",
    "**Note**: to produce the following visualization you will need to install the [Graphviz software](https://www.graphviz.org/download/) and the python package to use it from here. You can download the former at the provided link and install the Python Package by doing `pip install graphviz` in the terminal. Be sure to add Graphviz to the system variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28abb75-1824-4d76-bf0b-bd5afceec77b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from graphviz import Source\n",
    "from sklearn.tree import export_graphviz\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    "    mean_squared_error,\n",
    "    accuracy_score,\n",
    ")\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"trees\"\n",
    "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
    "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
    "\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2d23c3-2f81-481e-9868-b997dcbb9910",
   "metadata": {},
   "source": [
    "# The classification toy problem\n",
    "Let's observe the dataset first. A detailed information can be found [here](https://archive.ics.uci.edu/ml/datasets/iris)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c249390c-e4ff-48f6-9d88-4d1c6c23cd57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931585cb-7fed-4376-8e66-25825d86fa0c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    data=np.c_[iris[\"data\"], iris[\"target\"]], columns=iris[\"feature_names\"] + [\"target\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601d8662-6e5f-46c7-ae4b-803f358c60c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df[\"target\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9fd8f8-d208-4c8a-9864-171a405e16d0",
   "metadata": {},
   "source": [
    "**Why do we say that this dataset is suitable for a classification problem?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ff5522-86c1-41fd-807d-9f7f5114956f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df[\"target\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6b0798-7aff-4a37-b2a2-1007b7d475a6",
   "metadata": {},
   "source": [
    "# Classification\n",
    "\n",
    "[Decision Tree Classifier doc](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf564a1-1bc2-4dd6-867c-cda2cf765125",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = iris.data[:, 2:]  # petal length and width\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f617d1-da61-4b3c-b061-e71d0c14052d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "iris.data[:, 2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179669e9-edfc-4894-8800-6f2389d0f021",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "(X[:,0] == 2.5).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a57fbfb-a936-4c97-98d0-56ac5f577fd6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tree_clf = DecisionTreeClassifier(max_depth=2, random_state=42)\n",
    "tree_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26536f75-a5b4-4d53-a5dc-5798787b3d45",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "export_graphviz(\n",
    "    tree_clf,\n",
    "    out_file=os.path.join(IMAGES_PATH, \"iris_tree.dot\"),\n",
    "    feature_names=iris.feature_names[2:],\n",
    "    class_names=iris.target_names,\n",
    "    rounded=True,\n",
    "    filled=True,\n",
    ")\n",
    "\n",
    "Source.from_file(os.path.join(IMAGES_PATH, \"iris_tree.dot\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3357333f-c0d0-4982-9415-fd294e428a9e",
   "metadata": {},
   "source": [
    "**Scikit-learn's CART Algorithm and Split Points**\n",
    "\n",
    "In scikit-learn's implementation of the CART (Classification and Regression Trees) algorithm, split points are determined as the average of consecutive, unique values of the feature under consideration for a split. Specifically, if two adjacent unique values in a sorted feature column are \\(a\\) and \\(b\\), the potential split point \\(s\\) is calculated as:\n",
    "\n",
    "\\[\n",
    "s = \\frac{a + b}{2}\n",
    "\\]\n",
    "\n",
    "**Implications**\n",
    "\n",
    "Although the calculated split point may not be a value present in the original dataset, it is still influenced by the dataset's unique feature values. This ensures that the calculated split point is representative of the range and distribution of the feature values in the dataset.\n",
    "\n",
    "**Other Implementations**\n",
    "\n",
    "It's worth mentioning that other implementations of decision tree algorithms may directly use unique feature values as split points. The choice between these methods can depend on various factors, including the specific problem being solved and computational efficiency considerations.\n",
    "\n",
    "Look at the cell below to see this in practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31318f51-05f2-4e73-b69b-b9002ab59764",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# the split point on top of the decision tree above is 2.45 for petal width (first feature)\n",
    "\n",
    "np.sort(X[:,0]) # by ordering all the values we observe that the split point is between 1.9 and 3, whose average is exactly 2.45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e245486b-ef6f-4929-a441-07b076740225",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundary(\n",
    "    clf, X, y, axes=[0, 7.5, 0, 3], iris=True, legend=False, plot_training=True\n",
    "):\n",
    "    x1s = np.linspace(axes[0], axes[1], 100)\n",
    "    x2s = np.linspace(axes[2], axes[3], 100)\n",
    "    x1, x2 = np.meshgrid(x1s, x2s)\n",
    "    X_new = np.c_[x1.ravel(), x2.ravel()]\n",
    "    y_pred = clf.predict(X_new).reshape(x1.shape)\n",
    "    custom_cmap = ListedColormap([\"#fafab0\", \"#9898ff\", \"#a0faa0\"])\n",
    "    plt.contourf(x1, x2, y_pred, alpha=0.3, cmap=custom_cmap)\n",
    "    if not iris:\n",
    "        custom_cmap2 = ListedColormap([\"#7d7d58\", \"#4c4c7f\", \"#507d50\"])\n",
    "        plt.contour(x1, x2, y_pred, cmap=custom_cmap2, alpha=0.8)\n",
    "    if plot_training:\n",
    "        plt.plot(X[:, 0][y == 0], X[:, 1][y == 0], \"yo\", label=\"Iris setosa\")\n",
    "        plt.plot(X[:, 0][y == 1], X[:, 1][y == 1], \"bs\", label=\"Iris versicolor\")\n",
    "        plt.plot(X[:, 0][y == 2], X[:, 1][y == 2], \"g^\", label=\"Iris virginica\")\n",
    "        plt.axis(axes)\n",
    "    if iris:\n",
    "        plt.xlabel(\"Petal length\", fontsize=14)\n",
    "        plt.ylabel(\"Petal width\", fontsize=14)\n",
    "    else:\n",
    "        plt.xlabel(r\"$x_1$\", fontsize=18)\n",
    "        plt.ylabel(r\"$x_2$\", fontsize=18, rotation=0)\n",
    "    if legend:\n",
    "        plt.legend(loc=\"lower right\", fontsize=14)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plot_decision_boundary(tree_clf, X, y)\n",
    "plt.plot([2.45, 2.45], [0, 3], \"k-\", linewidth=2)\n",
    "plt.plot([2.45, 7.5], [1.75, 1.75], \"k--\", linewidth=2)\n",
    "plt.plot([4.95, 4.95], [0, 1.75], \"k:\", linewidth=2)\n",
    "plt.plot([4.85, 4.85], [1.75, 3], \"k:\", linewidth=2)\n",
    "plt.text(1.40, 1.0, \"Depth=0\", fontsize=15)\n",
    "plt.text(3.2, 1.80, \"Depth=1\", fontsize=13)\n",
    "plt.text(4.05, 0.5, \"(Depth=2)\", fontsize=11)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913fc120-a5db-4289-8aad-18d321bf0521",
   "metadata": {},
   "source": [
    "## Predicting classes and class probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8134fc9a-358e-46a4-ac36-0943b7285668",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269ff1c8-68bb-4b91-bf54-a120785452c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd4a30c-65bc-4408-94c4-d66deebc1784",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "iris.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9d1c72-8f82-46bd-b9ee-6a95cd1e994d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72406129-1799-49e5-b40c-1255e22316ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_clf.predict_proba([[1.5, 2.0], [4.5, 1.0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589d63fe-ad6a-43bf-a2d8-d87d860fd971",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_clf.predict([[5, 1.5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54bf9c80-2f6c-4d8e-9400-d9718218a453",
   "metadata": {},
   "source": [
    "# Train and test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bafe7c6-5950-47a6-9d71-069df1c3230b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ab0ce6-0723-4540-930e-cd9cf49bfdb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39d5c7a-2cdb-44c1-8341-af3975dc06a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21a6f13-a24d-4884-a9dd-5d1eba810f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e50a84d-d11c-4f28-8ad0-9a9161ca0988",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_clf = DecisionTreeClassifier(max_depth=2, random_state=42)\n",
    "tree_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91f2de2-b52d-4677-8c52-c03b988d76d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = tree_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9d3545-11f9-4f7a-bd09-465ef22009d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6c7093-8e35-4512-9c79-311cc81e7081",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_test, y_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812ff166-d6e1-427c-afff-ea5017711011",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test, y_hat).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b8ad88-9762-4f2f-b09f-8d337a769003",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74033f17-7b74-45cf-befb-b3eeee545c15",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9259cb2-37f3-4f0f-a1ab-a7c2deef8c78",
   "metadata": {},
   "source": [
    "# Sensitivity to training set details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393f8e06-b51c-4703-97b0-9002fff77af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b169e7-24f5-4532-a7fe-35f3f3f6b36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X[(X[:, 1] == X[:, 1][y == 1].max()) & (y == 1)]  # widest Iris versicolor flower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc51f80f-4c7b-4bdb-abd5-ff43897f8927",
   "metadata": {},
   "outputs": [],
   "source": [
    "not_widest_versicolor = (X[:, 1] != 1.8) | (y == 2)\n",
    "X_tweaked = X[not_widest_versicolor]\n",
    "y_tweaked = y[not_widest_versicolor]\n",
    "\n",
    "tree_clf_tweaked = DecisionTreeClassifier(max_depth=2, random_state=40)\n",
    "tree_clf_tweaked.fit(X_tweaked, y_tweaked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6f9663-7a4b-493d-a781-2265b10344b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tweaked.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b29e2b-798e-4365-a1ba-5fbc2fa01130",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 4))\n",
    "plot_decision_boundary(tree_clf_tweaked, X_tweaked, y_tweaked, legend=False)\n",
    "plt.plot([0, 7.5], [0.8, 0.8], \"k-\", linewidth=2)\n",
    "plt.plot([0, 7.5], [1.75, 1.75], \"k--\", linewidth=2)\n",
    "plt.text(1.0, 0.9, \"Depth=0\", fontsize=15)\n",
    "plt.text(1.0, 1.80, \"Depth=1\", fontsize=13)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1bfdc87-43d8-4097-8907-1386bd365c15",
   "metadata": {},
   "source": [
    "## Another toy example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9b2112-4caf-4b90-ad03-a1cba6758c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xm, ym = make_moons(n_samples=100, noise=0.25, random_state=53)\n",
    "\n",
    "deep_tree_clf1 = DecisionTreeClassifier(random_state=42)\n",
    "deep_tree_clf2 = DecisionTreeClassifier(min_samples_leaf=20, random_state=42)\n",
    "deep_tree_clf1.fit(Xm, ym)\n",
    "deep_tree_clf2.fit(Xm, ym)\n",
    "\n",
    "fig, axes = plt.subplots(ncols=2, figsize=(10, 4), sharey=True)\n",
    "plt.sca(axes[0])\n",
    "plot_decision_boundary(deep_tree_clf1, Xm, ym, axes=[-1.5, 2.4, -1, 1.5], iris=False)\n",
    "plt.title(\"No restrictions\", fontsize=16)\n",
    "plt.sca(axes[1])\n",
    "plot_decision_boundary(deep_tree_clf2, Xm, ym, axes=[-1.5, 2.4, -1, 1.5], iris=False)\n",
    "plt.title(\"min_samples_leaf = {}\".format(deep_tree_clf2.min_samples_leaf), fontsize=14)\n",
    "plt.ylabel(\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8e588f-f9dd-44a2-a63b-e895edb39caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "angle = np.pi / 180 * 20\n",
    "rotation_matrix = np.array(\n",
    "    [[np.cos(angle), -np.sin(angle)], [np.sin(angle), np.cos(angle)]]\n",
    ")\n",
    "Xr = X.dot(rotation_matrix)\n",
    "\n",
    "tree_clf_r = DecisionTreeClassifier(random_state=42)\n",
    "tree_clf_r.fit(Xr, y)\n",
    "\n",
    "plt.figure(figsize=(8, 3))\n",
    "plot_decision_boundary(tree_clf_r, Xr, y, axes=[0.5, 7.5, -1.0, 1], iris=False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f96b4b9-4f46-4b56-9080-c6b49634fad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(6)\n",
    "Xs = np.random.rand(100, 2) - 0.5\n",
    "ys = (Xs[:, 0] > 0).astype(np.float32) * 2\n",
    "\n",
    "angle = np.pi / 4\n",
    "rotation_matrix = np.array(\n",
    "    [[np.cos(angle), -np.sin(angle)], [np.sin(angle), np.cos(angle)]]\n",
    ")\n",
    "Xsr = Xs.dot(rotation_matrix)\n",
    "\n",
    "tree_clf_s = DecisionTreeClassifier(random_state=42)\n",
    "tree_clf_s.fit(Xs, ys)\n",
    "tree_clf_sr = DecisionTreeClassifier(random_state=42)\n",
    "tree_clf_sr.fit(Xsr, ys)\n",
    "\n",
    "fig, axes = plt.subplots(ncols=2, figsize=(10, 4), sharey=True)\n",
    "plt.sca(axes[0])\n",
    "plot_decision_boundary(tree_clf_s, Xs, ys, axes=[-0.7, 0.7, -0.7, 0.7], iris=False)\n",
    "plt.sca(axes[1])\n",
    "plot_decision_boundary(tree_clf_sr, Xsr, ys, axes=[-0.7, 0.7, -0.7, 0.7], iris=False)\n",
    "plt.ylabel(\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d20c746-45a4-4b93-95fe-aad9da4cfb57",
   "metadata": {},
   "source": [
    "# Regression trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e844dd4b-5c97-4796-9f37-00d81a508eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quadratic training set + noise\n",
    "# np.random.seed(42)\n",
    "rng = np.random.RandomState(42)\n",
    "m = 200\n",
    "X = rng.rand(m, 1)\n",
    "y = 4 * (X - 0.5) ** 2\n",
    "y = y + np.random.randn(m, 1) / 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74cd64ad-fac6-405c-9085-2bb589fb5bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "X[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece31161-bc8c-4178-aac9-d1d4af63ef2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    print(rng.rand(m, 1).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb65022c-5c85-4b8c-8242-55eaa5e38bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_reg = DecisionTreeRegressor(max_depth=2, random_state=42)\n",
    "tree_reg.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db84e44-e905-4cce-b004-18c98ffeaab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_reg1 = DecisionTreeRegressor(random_state=42, max_depth=2)\n",
    "tree_reg2 = DecisionTreeRegressor(random_state=42, max_depth=3)\n",
    "tree_reg1.fit(X, y)\n",
    "tree_reg2.fit(X, y)\n",
    "\n",
    "\n",
    "def plot_regression_predictions(tree_reg, X, y, axes=[0, 1, -0.2, 1], ylabel=\"$y$\"):\n",
    "    x1 = np.linspace(axes[0], axes[1], 500).reshape(-1, 1)\n",
    "    y_pred = tree_reg.predict(x1)\n",
    "    plt.axis(axes)\n",
    "    plt.xlabel(\"$x_1$\", fontsize=18)\n",
    "    if ylabel:\n",
    "        plt.ylabel(ylabel, fontsize=18, rotation=0)\n",
    "    plt.plot(X, y, \"b.\")\n",
    "    plt.plot(x1, y_pred, \"r.-\", linewidth=2, label=r\"$\\hat{y}$\")\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(ncols=2, figsize=(10, 4), sharey=True)\n",
    "plt.sca(axes[0])\n",
    "plot_regression_predictions(tree_reg1, X, y)\n",
    "for split, style in ((0.1973, \"k-\"), (0.0917, \"k--\"), (0.7718, \"k--\")):\n",
    "    plt.plot([split, split], [-0.2, 1], style, linewidth=2)\n",
    "plt.text(0.21, 0.65, \"Depth=0\", fontsize=15)\n",
    "plt.text(0.01, 0.2, \"Depth=1\", fontsize=13)\n",
    "plt.text(0.65, 0.8, \"Depth=1\", fontsize=13)\n",
    "plt.legend(loc=\"upper center\", fontsize=18)\n",
    "plt.title(\"max_depth=2\", fontsize=14)\n",
    "\n",
    "plt.sca(axes[1])\n",
    "plot_regression_predictions(tree_reg2, X, y, ylabel=None)\n",
    "for split, style in ((0.1973, \"k-\"), (0.0917, \"k--\"), (0.7718, \"k--\")):\n",
    "    plt.plot([split, split], [-0.2, 1], style, linewidth=2)\n",
    "for split in (0.0458, 0.1298, 0.2873, 0.9040):\n",
    "    plt.plot([split, split], [-0.2, 1], \"k:\", linewidth=1)\n",
    "plt.text(0.3, 0.5, \"Depth=2\", fontsize=13)\n",
    "plt.title(\"max_depth=3\", fontsize=14)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0adabcc9-8a50-4bf6-bece-e7f3b3540d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "export_graphviz(\n",
    "    tree_reg1,\n",
    "    out_file=os.path.join(IMAGES_PATH, \"regression_tree.dot\"),\n",
    "    feature_names=[\"x1\"],\n",
    "    rounded=True,\n",
    "    filled=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de285013-e0ad-4561-bf2a-2d7bbbf4a5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Source.from_file(os.path.join(IMAGES_PATH, \"regression_tree.dot\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74fa4d97-0a62-4070-b8fd-3369b5c45b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_reg1 = DecisionTreeRegressor(random_state=42)\n",
    "tree_reg2 = DecisionTreeRegressor(random_state=42, min_samples_leaf=10)\n",
    "tree_reg1.fit(X, y)\n",
    "tree_reg2.fit(X, y)\n",
    "\n",
    "x1 = np.linspace(0, 1, 500).reshape(-1, 1)\n",
    "y_pred1 = tree_reg1.predict(x1)\n",
    "y_pred2 = tree_reg2.predict(x1)\n",
    "\n",
    "fig, axes = plt.subplots(ncols=2, figsize=(10, 4), sharey=True)\n",
    "\n",
    "plt.sca(axes[0])\n",
    "plt.plot(X, y, \"b.\")\n",
    "plt.plot(x1, y_pred1, \"r.-\", linewidth=2, label=r\"$\\hat{y}$\")\n",
    "plt.axis([0, 1, -0.2, 1.1])\n",
    "plt.xlabel(\"$x_1$\", fontsize=18)\n",
    "plt.ylabel(\"$y$\", fontsize=18, rotation=0)\n",
    "plt.legend(loc=\"upper center\", fontsize=18)\n",
    "plt.title(\"No restrictions\", fontsize=14)\n",
    "\n",
    "plt.sca(axes[1])\n",
    "plt.plot(X, y, \"b.\")\n",
    "plt.plot(x1, y_pred2, \"r.-\", linewidth=2, label=r\"$\\hat{y}$\")\n",
    "plt.axis([0, 1, -0.2, 1.1])\n",
    "plt.xlabel(\"$x_1$\", fontsize=18)\n",
    "plt.title(\"min_samples_leaf={}\".format(tree_reg2.min_samples_leaf), fontsize=14)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38b47cf-4aec-4e1d-8864-f56e64e5be5b",
   "metadata": {},
   "source": [
    "# Predicting S&P500 Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801b3458-e999-4c0e-b368-ecf6ffca924d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_lags(df, columns, n_lags=1):\n",
    "    \"\"\"\n",
    "    Add lags to specific columns in a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - df (DataFrame): Original DataFrame.\n",
    "    - columns (list): List of column names for which to create lags.\n",
    "    - n_lags (int): Number of lags to create for each column.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame: Updated DataFrame with lag columns.\n",
    "    \"\"\"\n",
    "    for column in columns:\n",
    "        for lag in range(1, n_lags + 1):\n",
    "            df[f\"{column}_lag{lag}\"] = df[column].shift(lag)\n",
    "    return df\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"data\\Rv_daily_lec4.csv\", index_col=0)\n",
    "\n",
    "col_to_transform = [\"TBill3M\", \"TBill1Y\", \"Oil\", \"Gold\", \"SP_volume\"]\n",
    "for c in col_to_transform:\n",
    "    df[\"{}_ret\".format(c)] = df[c].pct_change(1) * 100\n",
    "df = df.dropna()\n",
    "\n",
    "df = add_lags(df, [\"Return_close\"], n_lags=3)\n",
    "df = df.dropna()\n",
    "df = df.replace([np.inf, -np.inf], 0)\n",
    "\n",
    "df[\"Ret_binary\"] = (df[\"Return_close\"] > 0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591ef32a-e6cf-4465-aecd-d216b86d6c64",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf8d157-1eee-4e83-83ff-7b9ec698b640",
   "metadata": {},
   "source": [
    "## Regression Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94f7c3e-dbfa-4802-a676-391f7f322304",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Select features (replace with actual feature names)\n",
    "features = [\"RV\", \"TBill1Y_ret\"]\n",
    "\n",
    "# Splitting the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df[features], df[\"Return_close\"], test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Initialize Regression Tree Model\n",
    "regression_tree_model = DecisionTreeRegressor()\n",
    "\n",
    "# Fit the model\n",
    "regression_tree_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_reg = regression_tree_model.predict(X_test)\n",
    "\n",
    "mse_regression = mean_squared_error(y_test, y_pred_reg)\n",
    "print(f\"Mean Squared Error for Regression: {mse_regression}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73887552-0841-4a02-ae79-35ef212a3fbc",
   "metadata": {},
   "source": [
    "## Classification Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb225cd-ff4b-4227-b1e6-6c029a5e439d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6ed97f-d906-4244-8ed1-d0babf402ed7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Select features (replace with actual feature names)\n",
    "features = [\n",
    "    \"weekday\",\n",
    "    \"TBill3M_ret\",\n",
    "    \"TBill1Y_ret\",\n",
    "    \"Oil_ret\",\n",
    "    \"Gold_ret\",\n",
    "    \"SP_volume_ret\",\n",
    "    \"Return_close_lag1\",\n",
    "    \"Return_close_lag2\",\n",
    "    \"Return_close_lag3\",\n",
    "]\n",
    "# Splitting the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df[features], df[\"Ret_binary\"], test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Initialize Decision Tree Model\n",
    "decision_tree_model = DecisionTreeClassifier()\n",
    "\n",
    "# Fit the model\n",
    "decision_tree_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_class = decision_tree_model.predict(X_test)\n",
    "\n",
    "accuracy_classification = accuracy_score(y_test, y_pred_class)\n",
    "print(f\"Accuracy for Classification: {accuracy_classification}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74eecf9d-7cc6-4edb-90df-7bab4eba7362",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Select features (replace with actual feature names)\n",
    "features = [\n",
    "    \"RV\",\n",
    "    \"weekday\",\n",
    "    \"TBill3M_ret\",\n",
    "    \"TBill1Y_ret\",\n",
    "    \"Oil_ret\",\n",
    "    \"Gold_ret\",\n",
    "    \"SP_volume_ret\",\n",
    "    \"Return_close_lag1\",\n",
    "    \"Return_close_lag2\",\n",
    "    \"Return_close_lag3\",\n",
    "]\n",
    "\n",
    "# Splitting the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df[features], df[\"Ret_binary\"], test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Initialize Random Forest Model\n",
    "random_forest_model = RandomForestClassifier(\n",
    "    criterion=\"log_loss\", max_depth=25, min_samples_leaf=100, random_state=35\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "random_forest_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_class = random_forest_model.predict(X_test)\n",
    "\n",
    "accuracy_classification = accuracy_score(y_test, y_pred_class)\n",
    "print(f\"Accuracy for Classification: {accuracy_classification}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab68ba8-f609-4cff-87a6-286af0ae9d16",
   "metadata": {},
   "source": [
    "# Issues, weaknesses and things to remark\n",
    "\n",
    "**Categorical Features**\n",
    "When splitting a feature having $q$ possible unordered values, there are $2^{q-1}-1$ possible partitions of the $q$ values into two groups, and the computations become prohibitive for large $q$. However, with a $0-1$ outcome, this computation simplifies. We order the feature classes according to the proportion falling in outcome class 1. Then we split this feature as if it were an ordered feature.\n",
    "\n",
    "\n",
    "One can show this gives the optimal split, in terms of cross-entropy or Gini index, among all possible $2^{q-1}-1$ splits. This result also holds for a quantitative outcome and square error loss-the categories are ordered by increasing mean of the outcome. \n",
    "\n",
    "\n",
    "**The Loss Matrix**\n",
    "\n",
    "In classification problems, the consequences of misclassifying observations are more serious in some classes than others. For example, it is probably worse to predict that a person will not have a heart attack when he/she actually will, than vice versa. To account for this, we define a $K \\times K$ loss matrix $\\mathbf{L}$, with $L_{k k^{\\prime}}$ being the loss incurred for classifying a class $k$ observation as class $k^{\\prime}$. Typically no loss is incurred for correct classifications,\n",
    "\n",
    "**Linear Combination Splits**\n",
    "Rather than restricting splits to be of the form $X_{j} \\leq s$, one can allow splits along linear combinations of the form $\\sum a_{j} X_{j} \\leq s$. The weights $a_{j}$ and split point $s$ are optimized to minimize the relevant criterion (such as the Gini index). While this can improve the predictive power of the tree, it can hurt *interpretability*. Computationally, the discreteness of the split point search precludes the use of a smooth optimization for the weights. A better way to incorporate linear combination splits is in the hierarchical mixtures of experts (HME) model (Section 9.5 of the book)\n",
    "\n",
    "\n",
    "**Instability of Trees**\n",
    "One major problem with trees is their *high variance*. Often a small change in the data can result in a very different series of splits, making interpretation somewhat precarious. The major reason for this instability is the hierarchical nature of the process: the effect of an error in the top split is propagated down to all of the splits below it. One can alleviate this to some degree by trying to use a more stable split criterion, but the inherent instability is not removed. It is the price to be paid for estimating a simple, tree-based structure from the data. \n",
    "\n",
    "\n",
    "**Lack of Smoothness**\n",
    "Another limitation of trees is the lack of smoothness of the prediction surface. In classification with $0 / 1$ loss, this doesn't hurt much, since bias in estimation of the class probabilities has a limited effect. However, this can degrade performance in the regression setting, where we would normally expect the underlying function to be smooth. \n",
    "\n",
    "**Difficulty in Capturing Additive Structure**\n",
    "Another problem with trees is their difficulty in modeling additive structure. In regression, suppose, for example, that $Y=c_{1} I\\left(X_{1}<t_{1}\\right)+c_{2} I\\left(X_{2}<\\right.$ $\\left.t_{2}\\right)+\\varepsilon$ where $\\varepsilon$ is zero-mean noise. Then a binary tree might make its first split on $X_{1}$ near $t_{1}$. At the next level down it would have to split both nodes on $X_{2}$ at $t_{2}$ in order to capture the additive structure. This might happen with sufficient data, but the model is given no special encouragement to find such structure. If there were ten rather than two additive effects, it would take many fortuitous splits to recreate the structure, and the data analyst would be hard pressed to recognize it in the estimated tree. The \"blame\" here can again be attributed to the binary tree structure, which has both advantages and drawbacks. Again the MARS method (Section 9.4) gives up this tree structure in order to capture additive structure.\n",
    "\n",
    "**Why do we talk about trees if they have some many problems?**\n",
    "\n",
    "Trees are weak learners (do you know what does it mean?).\n",
    "\n",
    "Being weak learners still means that they can be used in **ensemble** models and their simple but effective reasoning can be leveraged. In the next lecture we will introduce two techniques:\n",
    "- **Bagging** with Random Forest, averaging many trees.\n",
    "- **Boosting** with Adaboost, repeatedly training weak learners to improve classification/regression.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
