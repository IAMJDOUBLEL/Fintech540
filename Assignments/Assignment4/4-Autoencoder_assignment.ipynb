{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3ab9b3c-432a-4e97-8555-f4ed7c4c61f2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "# Fourth Assignment - FINTECH 540 - Machine Learning for FinTech - Dimensionality Reduction with Autoencoders\n",
    "\n",
    "In this assignment, you will attempt to replicate the S&P 500 index using the price series of some of its constituents. This task involves applying machine learning techniques, specifically neural networks, to select a subset of companies that tracks the index value well. You may also explore using a Principal Component Analysis (PCA) as a benchmarking tool, though this is not mandatory. The primary objective of this task is to achieve a satisfactory performance on the test set (out-of-sample). For a reference on the meaning of this exercise, refer to notebook 14 of our class material.\n",
    "\n",
    "## Dataset Overview\n",
    "\n",
    "- **Asset Close Prices**: 360 stocks + S&P500 Index (Last column of both files) from 2000 to 2023.\n",
    "- **Format**: Divided into train and test sets (2 files are provided). Do not split again into train and test. Split only the train set if you want to obtain a validation set for hyperparameter selection.\n",
    "\n",
    "\n",
    "## Task and General Hints\n",
    "\n",
    "In this assignment, you are tasked with building an unsupervised learning model on equity data. Your primary goal is to ensure accurate out-of-sample reproduction of the given index (S&P500) through a subset of the constituents and evaluate them with the below-mentioned metric.\n",
    "\n",
    "To guide you through this process, consider breaking down your tasks into the following three phases:\n",
    "\n",
    "**Preprocessing**\n",
    "The dataset is already free of inconsistencies, missing values, or outliers. \n",
    "- **Data Splitting**: The dataset is partitioned, and two files are provided to you.\n",
    "\n",
    "**Model Selection**\n",
    "- This notebook focuses on using neural networks for index replication. You can experiment with the different neural network architectures (autoencoders) we have seen in class. Feel free to compare the performance against a PCA methodology. \n",
    "\n",
    "**Model Tuning and Evaluation**\n",
    "- Once you've selected a model, you'll want to fine-tune its parameters to achieve a good index tracking out-of-sample. You must also choose the number of companies used to reproduce the index dynamics.\n",
    "- You may adjust parameters manually or construct a routine to fit several models with different hyperparameters. \n",
    "- Evaluate your final model using the function provided at the end of the notebook, paying attention to respect the indicated naming convention.\n",
    "\n",
    "**Note**: Parameter choices and tuning should be made thoughtfully while it is up to you. Carefully study the documentation of the neural network models and refer to the Jupyer Notebooks we used in class to see the possible parameters you can fine-tune.\n",
    "\n",
    "**IMPORTANT REMARK**: \n",
    "You must use the test set solely as data the model has never seen before. The results on that part of the dataset are those that are going to provide your grade.\n",
    "\n",
    "Remember to set the seed when training and instantiating your model. You can use either Keras (Tensorflow) or Pytorch for this task, and you must make your results fully reproducible for grading. Double-check that you have correctly set the seed before diving into the coding part.\n",
    "\n",
    "- [Setting the seed in Keras](https://www.tensorflow.org/api_docs/python/tf/keras/utils/set_random_seed)\n",
    "- [Setting the seed in Pytorch](https://pytorch.org/docs/stable/notes/randomness.html)\n",
    "\n",
    "# Grading Rubric\n",
    "\n",
    "Your grade for this assignment will be determined by a composite score that considers both the **normalized Root Mean Squared Error (RMSE)** on the test set and the **efficiency of your index reconstruction**. The formula for your final grade is as follows:\n",
    "\n",
    "$$ \\text{Final Score} = \\text{Weighted RMSE Score} + \\text{Weighted Efficiency Score} $$\n",
    "\n",
    "This will be a number between 0 and 100, with grades potentially curved before release.\n",
    "\n",
    "**Components of the Grading Rubric**\n",
    "\n",
    "1. **RMSE Score:**\n",
    "   - Calculated as:\n",
    "     $$ \\text{Normalized RMSE} = 1 - \\left( \\frac{\\text{RMSE}}{\\text{MAX_POSSIBLE_RMSE}} \\right) $$\n",
    "   - `MAX_POSSIBLE_RMSE` is set as the standard deviation of the target variable.\n",
    "   - RMSE measures how close your constructed index is to the actual index.\n",
    "   - This component contributes 70% to your final score.\n",
    "\n",
    "2. **Efficiency Score:**\n",
    "   - Calculated as:\n",
    "     $$ \\text{Efficiency Score} = 1 - \\left( \\frac{\\text{Number of Companies Used}}{\\text{Total Number of Companies}} \\right) $$\n",
    "   - Encourages models that use fewer companies for index replication. The more companies you use, the more costly it would be to construct that portfolio to track the S&P500, so the less, the better.\n",
    "   - This component contributes 30% to your final score.\n",
    "\n",
    "The final score is a weighted sum of the RMSE and Efficiency scores:\n",
    "\n",
    "$$ \\text{Final Score} = (\\text{Weight RMSE} \\times \\text{Normalized RMSE}) + (\\text{Weight Efficiency} \\times \\text{Efficiency Score}) $$\n",
    "\n",
    "Where:\n",
    "- `Weight RMSE` = 0.7 \n",
    "- `Weight Efficiency` = 0.3\n",
    "\n",
    "The final grade will be:\n",
    "\n",
    "$$ \\text{Grade} = \\lceil \\text{Final Score} \\times 100 \\rceil $$\n",
    "\n",
    "Rounded up to the nearest whole number.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9222e5ba-870d-4839-98b6-597bce2f00f8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "62/62 [==============================] - 0s 477us/step - loss: 0.1068\n",
      "Epoch 2/100\n",
      "62/62 [==============================] - 0s 440us/step - loss: 0.0200\n",
      "Epoch 3/100\n",
      "62/62 [==============================] - 0s 453us/step - loss: 0.0115\n",
      "Epoch 4/100\n",
      "62/62 [==============================] - 0s 463us/step - loss: 0.0084\n",
      "Epoch 5/100\n",
      "62/62 [==============================] - 0s 481us/step - loss: 0.0072\n",
      "Epoch 6/100\n",
      "62/62 [==============================] - 0s 528us/step - loss: 0.0068\n",
      "Epoch 7/100\n",
      "62/62 [==============================] - 0s 520us/step - loss: 0.0064\n",
      "Epoch 8/100\n",
      "62/62 [==============================] - 0s 488us/step - loss: 0.0060\n",
      "Epoch 9/100\n",
      "62/62 [==============================] - 0s 470us/step - loss: 0.0056\n",
      "Epoch 10/100\n",
      "62/62 [==============================] - 0s 472us/step - loss: 0.0052\n",
      "Epoch 11/100\n",
      "62/62 [==============================] - 0s 475us/step - loss: 0.0049\n",
      "Epoch 12/100\n",
      "62/62 [==============================] - 0s 457us/step - loss: 0.0046\n",
      "Epoch 13/100\n",
      "62/62 [==============================] - 0s 457us/step - loss: 0.0044\n",
      "Epoch 14/100\n",
      "62/62 [==============================] - 0s 452us/step - loss: 0.0042\n",
      "Epoch 15/100\n",
      "62/62 [==============================] - 0s 461us/step - loss: 0.0040\n",
      "Epoch 16/100\n",
      "62/62 [==============================] - 0s 450us/step - loss: 0.0038\n",
      "Epoch 17/100\n",
      "62/62 [==============================] - 0s 466us/step - loss: 0.0037\n",
      "Epoch 18/100\n",
      "62/62 [==============================] - 0s 464us/step - loss: 0.0036\n",
      "Epoch 19/100\n",
      "62/62 [==============================] - 0s 472us/step - loss: 0.0035\n",
      "Epoch 20/100\n",
      "62/62 [==============================] - 0s 474us/step - loss: 0.0034\n",
      "Epoch 21/100\n",
      "62/62 [==============================] - 0s 479us/step - loss: 0.0034\n",
      "Epoch 22/100\n",
      "62/62 [==============================] - 0s 474us/step - loss: 0.0033\n",
      "Epoch 23/100\n",
      "62/62 [==============================] - 0s 467us/step - loss: 0.0032\n",
      "Epoch 24/100\n",
      "62/62 [==============================] - 0s 487us/step - loss: 0.0031\n",
      "Epoch 25/100\n",
      "62/62 [==============================] - 0s 470us/step - loss: 0.0030\n",
      "Epoch 26/100\n",
      "62/62 [==============================] - 0s 463us/step - loss: 0.0030\n",
      "Epoch 27/100\n",
      "62/62 [==============================] - 0s 470us/step - loss: 0.0029\n",
      "Epoch 28/100\n",
      "62/62 [==============================] - 0s 470us/step - loss: 0.0028\n",
      "Epoch 29/100\n",
      "62/62 [==============================] - 0s 474us/step - loss: 0.0028\n",
      "Epoch 30/100\n",
      "62/62 [==============================] - 0s 475us/step - loss: 0.0027\n",
      "Epoch 31/100\n",
      "62/62 [==============================] - 0s 476us/step - loss: 0.0027\n",
      "Epoch 32/100\n",
      "62/62 [==============================] - 0s 453us/step - loss: 0.0027\n",
      "Epoch 33/100\n",
      "62/62 [==============================] - 0s 469us/step - loss: 0.0026\n",
      "Epoch 34/100\n",
      "62/62 [==============================] - 0s 486us/step - loss: 0.0026\n",
      "Epoch 35/100\n",
      "62/62 [==============================] - 0s 480us/step - loss: 0.0026\n",
      "Epoch 36/100\n",
      "62/62 [==============================] - 0s 476us/step - loss: 0.0026\n",
      "Epoch 37/100\n",
      "62/62 [==============================] - 0s 477us/step - loss: 0.0025\n",
      "Epoch 38/100\n",
      "62/62 [==============================] - 0s 475us/step - loss: 0.0025\n",
      "Epoch 39/100\n",
      "62/62 [==============================] - 0s 471us/step - loss: 0.0025\n",
      "Epoch 40/100\n",
      "62/62 [==============================] - 0s 484us/step - loss: 0.0025\n",
      "Epoch 41/100\n",
      "62/62 [==============================] - 0s 482us/step - loss: 0.0025\n",
      "Epoch 42/100\n",
      "62/62 [==============================] - 0s 474us/step - loss: 0.0025\n",
      "Epoch 43/100\n",
      "62/62 [==============================] - 0s 475us/step - loss: 0.0025\n",
      "Epoch 44/100\n",
      "62/62 [==============================] - 0s 482us/step - loss: 0.0025\n",
      "Epoch 45/100\n",
      "62/62 [==============================] - 0s 481us/step - loss: 0.0025\n",
      "Epoch 46/100\n",
      "62/62 [==============================] - 0s 473us/step - loss: 0.0025\n",
      "Epoch 47/100\n",
      "62/62 [==============================] - 0s 486us/step - loss: 0.0025\n",
      "Epoch 48/100\n",
      "62/62 [==============================] - 0s 484us/step - loss: 0.0025\n",
      "Epoch 49/100\n",
      "62/62 [==============================] - 0s 486us/step - loss: 0.0025\n",
      "Epoch 50/100\n",
      "62/62 [==============================] - 0s 481us/step - loss: 0.0025\n",
      "Epoch 51/100\n",
      "62/62 [==============================] - 0s 486us/step - loss: 0.0025\n",
      "Epoch 52/100\n",
      "62/62 [==============================] - 0s 477us/step - loss: 0.0025\n",
      "Epoch 53/100\n",
      "62/62 [==============================] - 0s 479us/step - loss: 0.0025\n",
      "Epoch 54/100\n",
      "62/62 [==============================] - 0s 478us/step - loss: 0.0025\n",
      "Epoch 55/100\n",
      "62/62 [==============================] - 0s 488us/step - loss: 0.0025\n",
      "Epoch 56/100\n",
      "62/62 [==============================] - 0s 483us/step - loss: 0.0025\n",
      "Epoch 57/100\n",
      "62/62 [==============================] - 0s 482us/step - loss: 0.0025\n",
      "Epoch 58/100\n",
      "62/62 [==============================] - 0s 489us/step - loss: 0.0025\n",
      "Epoch 59/100\n",
      "62/62 [==============================] - 0s 485us/step - loss: 0.0025\n",
      "Epoch 60/100\n",
      "62/62 [==============================] - 0s 492us/step - loss: 0.0025\n",
      "Epoch 61/100\n",
      "62/62 [==============================] - 0s 489us/step - loss: 0.0025\n",
      "Epoch 62/100\n",
      "62/62 [==============================] - 0s 483us/step - loss: 0.0025\n",
      "Epoch 63/100\n",
      "62/62 [==============================] - 0s 496us/step - loss: 0.0025\n",
      "Epoch 64/100\n",
      "62/62 [==============================] - 0s 484us/step - loss: 0.0025\n",
      "Epoch 65/100\n",
      "62/62 [==============================] - 0s 744us/step - loss: 0.0025\n",
      "Epoch 66/100\n",
      "62/62 [==============================] - 0s 486us/step - loss: 0.0025\n",
      "Epoch 67/100\n",
      "62/62 [==============================] - 0s 489us/step - loss: 0.0025\n",
      "Epoch 68/100\n",
      "62/62 [==============================] - 0s 495us/step - loss: 0.0025\n",
      "Epoch 69/100\n",
      "62/62 [==============================] - 0s 482us/step - loss: 0.0025\n",
      "Epoch 70/100\n",
      "62/62 [==============================] - 0s 488us/step - loss: 0.0025\n",
      "Epoch 71/100\n",
      "62/62 [==============================] - 0s 492us/step - loss: 0.0025\n",
      "Epoch 72/100\n",
      "62/62 [==============================] - 0s 494us/step - loss: 0.0025\n",
      "Epoch 73/100\n",
      "62/62 [==============================] - 0s 497us/step - loss: 0.0025\n",
      "Epoch 74/100\n",
      "62/62 [==============================] - 0s 492us/step - loss: 0.0025\n",
      "Epoch 75/100\n",
      "62/62 [==============================] - 0s 483us/step - loss: 0.0025\n",
      "Epoch 76/100\n",
      "62/62 [==============================] - 0s 498us/step - loss: 0.0025\n",
      "Epoch 77/100\n",
      "62/62 [==============================] - 0s 478us/step - loss: 0.0025\n",
      "Epoch 78/100\n",
      "62/62 [==============================] - 0s 478us/step - loss: 0.0025\n",
      "Epoch 79/100\n",
      "62/62 [==============================] - 0s 485us/step - loss: 0.0025\n",
      "Epoch 80/100\n",
      "62/62 [==============================] - 0s 488us/step - loss: 0.0025\n",
      "Epoch 81/100\n",
      "62/62 [==============================] - 0s 490us/step - loss: 0.0025\n",
      "Epoch 82/100\n",
      "62/62 [==============================] - 0s 489us/step - loss: 0.0025\n",
      "Epoch 83/100\n",
      "62/62 [==============================] - 0s 484us/step - loss: 0.0025\n",
      "Epoch 84/100\n",
      "62/62 [==============================] - 0s 477us/step - loss: 0.0025\n",
      "Epoch 85/100\n",
      "62/62 [==============================] - 0s 491us/step - loss: 0.0025\n",
      "Epoch 86/100\n",
      "62/62 [==============================] - 0s 477us/step - loss: 0.0025\n",
      "Epoch 87/100\n",
      "62/62 [==============================] - 0s 491us/step - loss: 0.0025\n",
      "Epoch 88/100\n",
      "62/62 [==============================] - 0s 488us/step - loss: 0.0025\n",
      "Epoch 89/100\n",
      "62/62 [==============================] - 0s 484us/step - loss: 0.0025\n",
      "Epoch 90/100\n",
      "62/62 [==============================] - 0s 473us/step - loss: 0.0025\n",
      "Epoch 91/100\n",
      "62/62 [==============================] - 0s 486us/step - loss: 0.0025\n",
      "Epoch 92/100\n",
      "62/62 [==============================] - 0s 487us/step - loss: 0.0025\n",
      "Epoch 93/100\n",
      "62/62 [==============================] - 0s 485us/step - loss: 0.0025\n",
      "Epoch 94/100\n",
      "62/62 [==============================] - 0s 481us/step - loss: 0.0025\n",
      "Epoch 95/100\n",
      "62/62 [==============================] - 0s 480us/step - loss: 0.0025\n",
      "Epoch 96/100\n",
      "62/62 [==============================] - 0s 483us/step - loss: 0.0025\n",
      "Epoch 97/100\n",
      "62/62 [==============================] - 0s 479us/step - loss: 0.0025\n",
      "Epoch 98/100\n",
      "62/62 [==============================] - 0s 485us/step - loss: 0.0025\n",
      "Epoch 99/100\n",
      "62/62 [==============================] - 0s 487us/step - loss: 0.0025\n",
      "Epoch 100/100\n",
      "62/62 [==============================] - 0s 486us/step - loss: 0.0025\n",
      "145/145 [==============================] - 0s 244us/step\n",
      "37/37 [==============================] - 0s 254us/step\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import tensorflow as tf\n",
    "import random as python_random\n",
    "\n",
    "np.random.seed(41)\n",
    "python_random.seed(41)\n",
    "tf.random.set_seed(41)\n",
    "\n",
    "train_data = pd.read_csv('sp_train.csv')\n",
    "test_data = pd.read_csv('sp_test.csv')\n",
    "train_data = train_data.drop(columns=['Date'])\n",
    "test_data = test_data.drop(columns=['Date'])\n",
    "\n",
    "index_train = train_data['S&P'].values\n",
    "index_test = test_data['S&P'].values\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(train_data)\n",
    "X_test = scaler.transform(test_data)\n",
    "\n",
    "index_scaler = MinMaxScaler()\n",
    "index_train = index_scaler.fit_transform(index_train.reshape(-1, 1))\n",
    "index_test = index_scaler.transform(index_test.reshape(-1, 1))\n",
    "index_train = index_train.reshape(-1)\n",
    "index_test = index_test.reshape(-1)\n",
    "\n",
    "input_layer = Input(shape=(X_train.shape[1],))\n",
    "encoding_dim = 16\n",
    "encoded = Dense(encoding_dim, activation='relu')(input_layer)\n",
    "decoded = Dense(X_train.shape[1], activation='linear')(encoded)\n",
    "autoencoder = Model(input_layer, decoded)\n",
    "autoencoder.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "autoencoder.fit(X_train, X_train,\n",
    "                epochs=100,\n",
    "                batch_size=75)\n",
    "\n",
    "X_predicted = autoencoder.predict(X_train)\n",
    "test_predicted = autoencoder.predict(X_test)\n",
    "\n",
    "error_train = np.mean(np.abs(X_train - X_predicted)**2, axis=0)\n",
    "\n",
    "ind = np.argsort(error_train)\n",
    "sort_error = error_train[ind]\n",
    "\n",
    "n = 4\n",
    "\n",
    "portfolio_train = X_predicted[:, ind[:n]]\n",
    "portfolio_test = test_predicted[:, ind[:n]]\n",
    "\n",
    "tracked_index_insample = np.mean(portfolio_train, axis=1)\n",
    "\n",
    "tracked_index_outofsample = np.mean(portfolio_test, axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a42438-a3f7-4d75-9fba-b756326b1dd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5542ce-60c6-4ebb-abea-ab5b9764c9b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f4502c-2d12-4f80-8d11-73f90b6a0414",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a7c9e9-2a3e-422b-886c-9f07f3714d20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f2dc1c-ceef-4851-9ee0-ef633bdbf0ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "31d33b67-700b-44ad-949c-d70ed2667150",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "**Instructions to let the next code cell run:**\n",
    "\n",
    "Before running the cell below, ensure the following:\n",
    "\n",
    "- The target variable of your problem has to be named exactly `index_test`, while the out-of-sample prediction variable has to be named `tracked_index_outofsample`. Store the number of companies to reconstruct the index dynamic in a variable called `n`. The calculation of the evaluation function relies on this naming convention to determine the final grade. Please make sure that both `index_test` and `tracked_index_outofsample` are numpy arrays of dimensionality (1158,) where 1158 is the length of the out-of-sample set. If the dimensions are different than that the following cell will not run.\n",
    "\n",
    "By adhering to these naming conventions, the grading cell can compute the final score without any issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "35dd6ac5-e25f-4866-94b6-3773918dda59",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The grade for this assignment is 94.00\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def evaluate_index_performance(y_test, y_pred, num_companies_used, total_companies=360, weight_rmse=0.7, weight_efficiency=0.3):\n",
    "    \"\"\"\n",
    "    Function to evaluate the performance of the reconstructed index.\n",
    "    \n",
    "    :param y_test: Actual index values (out of sample)\n",
    "    :param y_pred: Predicted index values using a subset of companies\n",
    "    :param num_companies_used: Number of companies used for the reconstruction\n",
    "    :param total_companies: Total number of companies in the index (default 500 for S&P 500)\n",
    "    :param weight_mse: Weight for the MSE score (default 0.7)\n",
    "    :param weight_efficiency: Weight for the efficiency score (default 0.3)\n",
    "    :return: A composite score combining MSE and efficiency\n",
    "    \"\"\"\n",
    "    # Calculate MSE and normalized MSE score\n",
    "    rmse = np.sqrt(np.mean((y_test - y_pred) ** 2))\n",
    "    max_possible_rmse = y_test.std()\n",
    "    rmse_score = 1 - rmse / max_possible_rmse\n",
    "\n",
    "    # Calculate efficiency score\n",
    "    efficiency_score = 1 - (num_companies_used / total_companies)\n",
    "\n",
    "    # Calculate final grade\n",
    "    final_score = weight_rmse * rmse_score + weight_efficiency * efficiency_score\n",
    "\n",
    "    return final_score\n",
    "\n",
    "grade = evaluate_index_performance(index_test, tracked_index_outofsample, n)\n",
    "print(f'The grade for this assignment is {math.ceil(grade * 100):.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f608e55e-a488-47d6-a2c7-61e46426133f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
